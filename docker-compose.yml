# =============================================================================
# n8n Production Stack — AWS EC2 t3.xlarge (4 vCPU, 16 GB RAM)
# =============================================================================
# Fully Podman-compatible (tested against podman-compose 1.x)
#
# Services:
#   - PostgreSQL 16  (database)
#   - Redis 7        (queue broker + AI chat memory)
#   - n8n            (workflow engine)
#   - n8n Runners    (external JS + Python task execution)
#   - Docling Serve  (document conversion API — CPU-only)
#   - Watchtower     (image update monitor)
#
# Usage:
#   podman-compose up -d
#   podman-compose logs -f
#   podman-compose down
#
# Podman compatibility notes:
#   - Direct bind mounts instead of named volumes with driver_opts
#     (driver_opts is silently ignored by podman-compose)
#   - Simple depends_on (no condition: service_healthy)
#     (broken in podman-compose; services handle retries internally)
#   - nickfedor/watchtower (maintained fork of archived containrrr/watchtower)
#   - Podman socket path configurable via PODMAN_SOCKET env var
# =============================================================================

version: "3.8"

networks:
  n8n-net:
    driver: bridge

services:
  # ---------------------------------------------------------------------------
  # PostgreSQL 16 — Production Tuned
  # ---------------------------------------------------------------------------
  # Memory budget: 4 GB (of 16 GB total)
  # CPU budget: 1.5 cores (of 4 total)
  # Key tuning: shared_buffers=1GB, work_mem=32MB, max_connections=60
  # ---------------------------------------------------------------------------
  postgres:
    image: docker.io/library/postgres:16-bookworm
    container_name: n8n-postgres
    restart: unless-stopped
    networks:
      - n8n-net
    # NOTE: No port mapping — PostgreSQL is only accessible within n8n-net
    # For debugging, temporarily add:  ports: ["127.0.0.1:5432:5432"]
    environment:
      TZ: Asia/Kolkata
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      # Direct bind mounts (podman-compose ignores driver_opts on named volumes)
      - ./postgres/data:/var/lib/postgresql/data:Z
      - ./postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro,Z
      # Init scripts run ONLY on first boot (empty data dir). Safe to keep mounted.
      - ./postgres/init:/docker-entrypoint-initdb.d:ro,Z
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    deploy:
      resources:
        limits:
          memory: 4g
          cpus: "1.5"
        reservations:
          memory: 2g
          cpus: "0.5"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER} -d ${DB_NAME} || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ---------------------------------------------------------------------------
  # Redis 7 — Queue Broker + AI Chat Memory
  # ---------------------------------------------------------------------------
  # Memory budget: 512 MB (of 16 GB total)
  # CPU budget: 0.25 cores (of 4 total)
  #
  # Redis serves TWO purposes in this stack:
  #
  # 1. QUEUE MODE (production scaling)
  #    Redis acts as a message broker for n8n queue mode, which separates
  #    workflow execution from the n8n editor UI. Official benchmarks show
  #    7x throughput increase (23 → 162 req/s) with 0% failure rate.
  #    Queue mode is NOT enabled by default — to enable later, uncomment
  #    EXECUTIONS_MODE=queue in the n8n service and add n8n-worker containers.
  #
  # 2. AI CHAT MEMORY (workflow-level)
  #    n8n's "Redis Chat Memory" node stores conversation context for AI
  #    agent workflows. Redis is fast but ephemeral — for durable long-term
  #    chat history, use the "Postgres Chat Memory" node (uses existing PG).
  #    Best practice: Redis for fast session context, Postgres for permanent.
  #
  # NOTE: Redis does NOT replace PostgreSQL as n8n's primary database.
  #       All workflow definitions, credentials, and execution logs stay in PG.
  #
  # Configuration rationale:
  #   --maxmemory 256mb       Cap memory usage (within 512MB container limit)
  #   --maxmemory-policy      allkeys-lru evicts least-recently-used keys
  #   --appendonly yes         Durable writes (AOF) — survives container restarts
  #   --save                  RDB snapshots for backup recovery
  #   --databases 4           DB 0: queue mode, DB 1: chat memory, DB 2-3: spare
  # ---------------------------------------------------------------------------
  redis:
    image: docker.io/library/redis:latest
    container_name: n8n-redis
    restart: unless-stopped
    networks:
      - n8n-net
    # NOTE: No port mapping — Redis is only accessible within n8n-net
    # For debugging, temporarily add:  ports: ["127.0.0.1:6379:6379"]
    volumes:
      - ./redis/data:/data:Z
    deploy:
      resources:
        limits:
          memory: 768m
          cpus: "0.3"
        reservations:
          cpus: "0.1"
          memory: 256m
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "2"
    command:
      - redis-server
            --requirepass ${REDIS_PASSWORD}
            --maxmemory 256mb
            --maxmemory-policy allkeys-lru
            --appendonly yes
            --appendfsync everysec
            --save 900 1
            --save 300 10
            --save 60 10000
            --tcp-backlog 511
            --timeout 300
            --tcp-keepalive 60
            --databases 4

  # ---------------------------------------------------------------------------
  # n8n — Main Application
  # ---------------------------------------------------------------------------
  # Memory budget: 6 GB (of 16 GB total)
  # CPU budget: 1.5 cores (of 4 total)
  # Node.js heap: 4096 MB (within 6 GB container limit)
  #
  # Startup: n8n retries DB connection internally on launch (built-in).
  # With restart: unless-stopped, even if PG isn't ready, n8n will
  # restart and successfully connect once PG is healthy.
  #
  # Redis connection:
  #   Redis is configured and available. Queue mode is NOT enabled by default.
  #   To enable queue mode later, uncomment EXECUTIONS_MODE=queue below.
  #   Redis Chat Memory is available immediately via the n8n node —
  #   use host "n8n-redis", port 6379, and the REDIS_PASSWORD.
  # ---------------------------------------------------------------------------
  n8n:
    image: docker.n8n.io/n8nio/n8n:stable
    container_name: n8n
    restart: unless-stopped
    networks:
      - n8n-net
    ports:
      - "5678:5678"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - postgres
      - redis
    environment:
      # --- Timezone ---
      TZ: Asia/Kolkata
      GENERIC_TIMEZONE: Asia/Kolkata

      # --- Database ---
      DB_TYPE: postgresdb
      DB_POSTGRESDB_HOST: n8n-postgres
      DB_POSTGRESDB_PORT: "5432"
      DB_POSTGRESDB_DATABASE: ${DB_NAME}
      DB_POSTGRESDB_USER: ${DB_USER}
      DB_POSTGRESDB_PASSWORD: ${DB_PASSWORD}
      DB_POSTGRESDB_SCHEMA: public
      DB_POSTGRESDB_POOL_SIZE: "20"

      # --- Redis (available for Chat Memory + Queue Mode) ---
      # These env vars configure n8n's Bull queue (backed by Redis).
      # Even without queue mode enabled, Redis Chat Memory node can
      # connect using these credentials via the n8n credential UI.
      QUEUE_BULL_REDIS_HOST: n8n-redis
      QUEUE_BULL_REDIS_PORT: "6379"
      QUEUE_BULL_REDIS_PASSWORD: ${REDIS_PASSWORD}
      QUEUE_BULL_REDIS_DB: "0"

      # --- Queue Mode (disabled by default — uncomment to enable) ---
      # Enables distributed execution: n8n dispatches jobs to Redis,
      # workers pick them up. Requires at least one n8n-worker container.
      # See: https://docs.n8n.io/hosting/scaling/queue-mode/
      #
      # EXECUTIONS_MODE: queue
      # QUEUE_HEALTH_CHECK_ACTIVE: "true"

      # --- Hosting ---
      N8N_HOST: ${N8N_HOST}
      N8N_PROTOCOL: https
      WEBHOOK_URL: https://${N8N_HOST}
      N8N_PORT: "5678"
      N8N_ENCRYPTION_KEY: ${N8N_ENCRYPTION_KEY}
      N8N_SECURE_COOKIE: "true"
      NODE_ENV: production

      # --- Node.js Performance ---
      # 4 GB heap within 6 GB container limit
      NODE_OPTIONS: "--max-old-space-size=4096"

      # --- Task Runners (External Mode) ---
      N8N_RUNNERS_ENABLED: "true"
      N8N_RUNNERS_MODE: external
      N8N_RUNNERS_BROKER_LISTEN_ADDRESS: "0.0.0.0"
      N8N_RUNNERS_AUTH_TOKEN: ${RUNNERS_AUTH_TOKEN}
      N8N_NATIVE_PYTHON_RUNNER: "true"

      # --- Execution Pruning ---
      EXECUTIONS_DATA_PRUNE: "true"
      EXECUTIONS_DATA_MAX_AGE: "336"  # 14 days
      EXECUTIONS_DATA_PRUNE_MAX_COUNT: "50000"

      # --- Logging ---
      N8N_LOG_LEVEL: warn
      N8N_LOG_OUTPUT: console

      # --- Diagnostics ---
      N8N_DIAGNOSTICS_ENABLED: "false"
    volumes:
      - ./n8n/data:/home/node/.n8n:Z
    deploy:
      resources:
        limits:
          memory: 6g
          cpus: "1.5"
        reservations:
          memory: 3g
          cpus: "0.5"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:5678/healthz || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ---------------------------------------------------------------------------
  # n8n Task Runners (JavaScript + Python)
  # ---------------------------------------------------------------------------
  # Memory budget: 3 GB (of 16 GB total)
  # CPU budget: 1.0 core (of 4 total)
  #
  # Startup: Runner auto-retries broker connection. If n8n isn't ready,
  # restart policy handles reconnection.
  # ---------------------------------------------------------------------------
  n8n-runners:
    build:
      context: ./runners
      dockerfile: Dockerfile
    image: n8n-runners-python:latest
    container_name: n8n-runners
    restart: unless-stopped
    networks:
      - n8n-net
    depends_on:
      - n8n
    environment:
      N8N_RUNNERS_TASK_BROKER_URI: http://n8n:5679
      N8N_RUNNERS_AUTH_TOKEN: ${RUNNERS_AUTH_TOKEN}
      N8N_RUNNERS_AUTO_SHUTDOWN_TIMEOUT: "60"
      N8N_RUNNERS_TASK_TIMEOUT: "600"
      N8N_RUNNERS_MAX_CONCURRENCY: "15"
      GENERIC_TIMEZONE: Asia/Kolkata
    deploy:
      resources:
        limits:
          memory: 3g
          cpus: "1.0"
        reservations:
          memory: 1g
          cpus: "0.25"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:5681/healthz || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ---------------------------------------------------------------------------
  # Docling Serve — Document Conversion API (CPU-only)
  # ---------------------------------------------------------------------------
  # Docling Serve wraps IBM's Docling document conversion library as a REST API.
  # Converts PDF, DOCX, PPTX, HTML, images → Markdown, JSON, DocTags.
  #
  # API Endpoints (accessible from n8n via HTTP Request node):
  #   POST /v1/convert/source  — convert from URL
  #   POST /v1/convert/file    — convert uploaded file
  #   GET  /health             — health check
  #   GET  /docs               — Swagger UI (API documentation)
  #   GET  /ui                 — Gradio web UI (for manual testing)
  #
  # Pre-baked models (already in container image — no download needed):
  #   - Layout analysis     (docling-layout-heron, RT-DETR based)
  #   - Table structure     (TableFormer accurate + fast modes)
  #   - Picture classifier  (DocumentFigureClassifier-v2.0, ViT)
  #   - Code & formula      (CodeFormulaV2)
  #   - EasyOCR             (English + Latin recognition models)
  #
  # Additional CPU-friendly models (auto-downloaded at first boot into
  # persistent HF cache volume — only downloaded once):
  #   - SmolVLM-256M-Instruct    (picture description, 256M params)
  #   - Granite-Docling-258M     (VLM full-page → DocTags, 258M params) ⭐
  #   - SmolDocling-256M         (VLM full-page → DocTags, 256M params)
  #
  # All models run on CPU. No GPU required.
  # Performance on t3.xlarge (CPU-only):
  #   Standard pipeline:  ~3s/page (layout + table + text extraction)
  #   With EasyOCR:       ~13s/page (OCR is the most expensive operation)
  #   With VLM convert:   ~20-40s/page (SmolDocling/GraniteDocling on CPU)
  #
  # n8n Workflow Integration:
  #   Use HTTP Request node → POST http://n8n-docling:5001/v1/convert/file
  #   Content-Type: multipart/form-data (for file upload)
  #   Or POST http://n8n-docling:5001/v1/convert/source with JSON body
  #   for URL-based conversion.
  # ---------------------------------------------------------------------------
  docling:
    image: quay.io/docling-project/docling-serve-cpu:latest
    container_name: n8n-docling
    restart: unless-stopped
    networks:
      - n8n-net
    ports:
      # Bound to localhost only — access via n8n-net internally,
      # or localhost:5001 on the host for debugging/Swagger UI
      - "127.0.0.1:5001:5001"
    environment:
      # --- Server Configuration ---
      DOCLING_SERVE_HOST: "0.0.0.0"
      DOCLING_SERVE_PORT: "5001"
      DOCLING_SERVE_ENABLE_UI: "true"

      # --- Engine Configuration ---
      # Local engine with 1 worker (safe for single Uvicorn process).
      # share_models=true reuses loaded models across requests (saves RAM).
      DOCLING_SERVE_ENG_KIND: local
      DOCLING_SERVE_ENG_LOC_NUM_WORKERS: "1"
      DOCLING_SERVE_ENG_LOC_SHARE_MODELS: "true"

      # --- Model Loading ---
      # Pre-load all pipeline models at boot to avoid cold-start delays
      DOCLING_SERVE_LOAD_MODELS_AT_BOOT: "true"

      # --- Timeouts ---
      # Generous timeouts for large documents processed on CPU
      DOCLING_SERVE_MAX_SYNC_WAIT: "600"
      DOCLING_SERVE_MAX_DOCUMENT_TIMEOUT: "900"

      # --- CPU Thread Tuning ---
      # Match vCPU count. Prevents over-subscription with other services.
      OMP_NUM_THREADS: "4"
      MKL_NUM_THREADS: "4"
      DOCLING_NUM_THREADS: "4"

      # --- IMPORTANT: Single Uvicorn worker ---
      # >1 workers with LocalOrchestrator causes "Task Not Found" 404 errors
      UVICORN_WORKERS: "1"

      # --- HuggingFace Model Cache ---
      # Models from HF are cached in a persistent volume.
      # SmolVLM, Granite-Docling, SmolDocling are downloaded here.
      HF_HOME: /home/default/.cache/huggingface
      TRANSFORMERS_CACHE: /home/default/.cache/huggingface/hub

    volumes:
      # Persistent HuggingFace model cache — survives container restarts
      # VLM models (SmolVLM, SmolDocling, GraniteDocling) are stored here
      - ./docling/hf-cache:/home/default/.cache/huggingface:Z
      # Persistent docling pipeline model artifacts cache
      - ./docling/models:/home/default/.cache/docling:Z
      # Shared directory for document input/output with n8n
      - ./docling/documents:/documents:Z

    # --- Resource Limits (COMMENTED OUT — enable when ready to constrain) ---
    # Suggested production values for t3.xlarge with Docling running
    # alongside the full n8n stack. When enabling these limits, also
    # rebalance other services:
    #   n8n:     6 GB → 4 GB  (NODE_OPTIONS → --max-old-space-size=3072)
    #   Runners: 3 GB → 2 GB
    #   This frees 3 GB for Docling's 4 GB limit within the 16 GB total.
    #
    # deploy:
    #   resources:
    #     limits:
    #       memory: 4g
    #       cpus: "2.0"
    #     reservations:
    #       memory: 2g
    #       cpus: "0.5"

    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:5001/health || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      # Long start_period: model loading at boot takes 60-120s on CPU
      start_period: 180s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "3"

  # ---------------------------------------------------------------------------
  # Watchtower — Update Monitor (notification only, NO auto-update)
  # ---------------------------------------------------------------------------
  watchtower:
    image: docker.io/nickfedor/watchtower:latest
    container_name: watchtower
    restart: unless-stopped
    environment:
      TZ: Asia/Kolkata
      WATCHTOWER_MONITOR_ONLY: "true"
      WATCHTOWER_POLL_INTERVAL: "86400"
      WATCHTOWER_LABEL_ENABLE: "true"
      WATCHTOWER_DEBUG: "false"
      WATCHTOWER_CLEANUP: "false"
      # --- Notifications (configure after n8n is stable) ---
      # Option A: Email
      # WATCHTOWER_NOTIFICATIONS: email
      # WATCHTOWER_NOTIFICATION_EMAIL_FROM: watchtower@iitbacr.space
      # WATCHTOWER_NOTIFICATION_EMAIL_TO: admin@iitbacr.space
      #
      # Option B: n8n webhook
      # WATCHTOWER_NOTIFICATIONS: shoutrrr
      # WATCHTOWER_NOTIFICATION_URL: generic+https://n8n.iitbacr.space/webhook/watchtower
    volumes:
      - ${PODMAN_SOCKET}:/var/run/docker.sock:ro
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
    deploy:
      resources:
        limits:
          memory: 128m
          cpus: "0.1"
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "2"
